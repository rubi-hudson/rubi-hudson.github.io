<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Rubi Hudson</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="keywords" content="Rubi Hudson, University of Toronto, economics, AI, artificial intelligence, AI safety, mechanism design">
	<link rel="icon" type="image/svg+xml" href="/img/uoft_icon.svg">
	<link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Titillium+Web:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
    <link rel="stylesheet" type="text/css" href="css/main.css">
	<script language="JavaScript" src="js/HideShow.js"></script>
</head>
<body>
    <div class="container">
        <div class="sidebar">
		<div class="sidebar-content">
        <div class="mobile-name">Rubi Hudson — Research</div>
        <div class="photo-info-container">
            <img src="img/rubi_hudson.jpg" alt="Rubi Hudson" class="profile-photo">
            <div class="sidebar-info">
                <nav>
                    <ul>
                        <li><a href="/">Home</a></li>
                        <li><a class="active" href="research.html">Research</a></li>
                        <li><a href="files/Hudson, Rubi - Academic CV.pdf">CV</a></li>
                        <li><a href="https://crossingtherubicon.substack.com/">Blog</a></li>
                    </ul>
                </nav>
                <div class="contact-info">
                    <p><img src="img/email_icon.svg" alt="Email"> Email: <a href="mailto:rubijhudson@gmail.com" class="social-link">rubijhudson@gmail.com</a></p>
                    <p><img src="img/github_icon.svg" alt="GitHub"> GitHub: <a href="https://github.com/rubi-hudson" class="social-link">rubi-hudson</a></p>
                    <p><img src="img/twitter_icon.svg" alt="Twitter"> Twitter: <a href="https://www.twitter.com/undo_hubris" class="social-link">@undo_hubris</a></p>
                </div>
            </div>
        </div>
    </div>
</div>
        <main>
            <h1>Rubi Hudson — Research</h1>
            <hr>
			<p class="paper-entry"><a href="https://arxiv.org/abs/2412.20732">Joint Scoring Rules: Zero-Sum Competition Avoids Performative Prediction</a></p>
			<p class="paper-follow-up"><span style="font-style: italic;">Rubi Hudson</span></p>
			<p class="paper-follow-up">Proceedings of the AAAI Conference on Artificial Intelligence, 2025</p>
			<p  class="paper-follow-up"><a href="https://arxiv.org/src/2412.20732/anc/Technical_Appendix.pdf" style="text-decoration: underline;">Technical Appendix</a>, <a href="https://github.com/rubi-hudson/Joint-Scoring-Rules" style="text-decoration: underline;">Code</a>, <a href="https://www.alignmentforum.org/posts/FFCDWx6qBdBds6jvL/safe-predictive-agents-with-joint-scoring-rules" style="text-decoration: underline;">Summary Post</a></p>
			<a id="jsrzscapp" href="javascript:expandIt(abjsrzscapp, jsrzscapp)" style="text-decoration: none;">
				&#9656 Abstract
			</a>
			</nobr>

			<div id="abjsrzscapp" style="display: none; padding-left: 18">
			<p style="padding-left: 30;">
			In a decision-making scenario, a principal could use conditional predictions from an expert agent to inform their choice. However, this approach would introduce a fundamental conflict of interest. An agent optimizing for predictive accuracy is incentivized to manipulate their principal towards more predictable actions, which prevents that principal from being able to deterministically select their true preference. We demonstrate that this impossibility result can be overcome through the joint evaluation of multiple agents. When agents are made to engage in zero-sum competition, their incentive to influence the action taken is eliminated, and the principal can identify and take the action they most prefer. We further prove that this zero-sum setup is unique, efficiently implementable, and applicable under stochastic choice. Experiments in a toy environment demonstrate that training on a zero-sum objective significantly enhances both predictive accuracy and principal utility, and can eliminate previously learned manipulative behavior.
			</p>
			</div>
            <p class="paper-entry"><a href="https://proceedings.mlr.press/v216/oesterheld23a.html">Incentivizing honest performative predictions with proper scoring rules</a></p>
			<p class="paper-follow-up">Caspar Oesterheld*, Johannes Treutlein*, Emery Cooper, and <span style="font-style: italic;">Rubi Hudson</span></p>
			<p class="paper-follow-up">Uncertainty in Artificial Intelligence, 2023</p>
			<a id="ihpppsr" href="javascript:expandIt(abihpppsr, ihpppsr)" style="text-decoration: none;">
				&#9656 Abstract
			</a>
			</nobr>

			<div id="abihpppsr" style="display: none; padding-left: 18">
			<p style="padding-left: 30;">
			Proper scoring rules incentivize experts to accurately report beliefs, assuming predictions cannot influence outcomes. We relax this assumption and investigate incentives when predictions are performative, ie, when they can influence the outcome of the prediction, such as when making public predictions about the stock market. We say a prediction is a fixed point if it accurately reflects the expert's beliefs after that prediction has been made. We show that in this setting, reports maximizing expected score generally do not reflect an expert's beliefs, and we give bounds on the inaccuracy of such reports. We show that, for binary predictions, if the influence of the expert's prediction on outcomes is bounded, it is possible to define scoring rules under which optimal reports are arbitrarily close to fixed points. However, this is impossible for predictions over more than two outcomes. We also perform numerical simulations in a toy setting, showing that our bounds are tight in some situations and that prediction error is often substantial (greater than 5-10%). Lastly, we discuss alternative notions of optimality, including performative stability, and show that they incentivize reporting fixed points.
			</p>
			</div>
			<p class="paper-entry"><a href="https://arxiv.org/abs/2302.00805">Conditioning predictive models: Risks and strategies</a></p>
			<p  class="paper-follow-up">Evan Hubinger, Adam Jermyn, Johannes Treutlein, <span style="font-style: italic;">Rubi Hudson</span>, and Kate Woolverton </p>
			<p class="paper-follow-up">arXiv, 2023</p>
			<a id="cpmrs" href="javascript:expandIt(abcpmrs, cpmrs)" style="text-decoration: none;">
				&#9656 Abstract
			</a>
			</nobr>

			<div id="abcpmrs" style="display: none; padding-left: 18">
			<p style="padding-left: 30;">
			Our intention is to provide a definitive reference on what it would take to safely make use of generative/predictive models in the absence of a solution to the Eliciting Latent Knowledge problem. Furthermore, we believe that large language models can be understood as such predictive models of the world, and that such a conceptualization raises significant opportunities for their safe yet powerful use via carefully conditioning them to predict desirable outputs. Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us. There are numerous potential solutions to such problems, however, primarily via carefully conditioning models to predict the things we want (e.g. humans) rather than the things we don't (e.g. malign AIs). Furthermore, due to the simplicity of the prediction objective, we believe that predictive models present the easiest inner alignment problem that we are aware of. As a result, we think that conditioning approaches for predictive models represent the safest known way of eliciting human-level and slightly superhuman capabilities from large language models and other similar future models.
			</p>
			</div>
			<h2>Under Review</h2>
			<p class="paper-entry">Superalignment Anti-Literature Review</p>
			<p  class="paper-follow-up">Michael Cohen*, <span style="font-style: italic;">Rubi Hudson</span>*, and Yoshua Bengio </p>
        </main>
    </div>
<script>
document.addEventListener('DOMContentLoaded', function() {
  const abstractToggles = document.querySelectorAll('.abstract-toggle');
  
  abstractToggles.forEach(toggle => {
    toggle.addEventListener('click', function() {
      this.parentElement.classList.toggle('active');
    });
  });
});
</script>
</body>
</html>